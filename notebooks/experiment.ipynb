{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fashion Image Classifier: Experiment Notebook\n",
    "\n",
    "**Research question:** How much does transfer learning improve over a from-scratch CNN for fashion product classification, and which pretrained backbone performs best?\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import wandb\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from src.config import (\n",
    "    STYLES_CSV, IMAGES_DIR, FIGURES_DIR, WANDB_PROJECT,\n",
    "    DEFAULT_BATCH_SIZE, DEFAULT_EPOCHS, DEFAULT_LR, DEFAULT_DROPOUT,\n",
    ")\n",
    "from src.data_loader import (\n",
    "    load_metadata, filter_top_categories, verify_images_exist,\n",
    "    encode_labels, split_data, build_dataloaders,\n",
    ")\n",
    "from src.models import build_model, MODEL_REGISTRY\n",
    "from src.training import train_model, evaluate\n",
    "from src.evaluation import (\n",
    "    compute_accuracy, compute_classification_report,\n",
    "    compute_confusion_matrix, compute_topk_accuracy,\n",
    ")\n",
    "from src.wandb_utils import init_wandb_run, log_confusion_matrix, log_sample_predictions\n",
    "\n",
    "load_dotenv()\n",
    "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and filter metadata\n",
    "df = load_metadata(STYLES_CSV)\n",
    "print(f\"Total rows in styles.csv: {len(df)}\")\n",
    "print(f\"Unique article types: {df['articleType'].nunique()}\")\n",
    "\n",
    "df = filter_top_categories(df)\n",
    "print(f\"\\nAfter filtering (>= 500 images): {len(df)} rows, {df['articleType'].nunique()} categories\")\n",
    "\n",
    "df = verify_images_exist(df, IMAGES_DIR)\n",
    "print(f\"After verifying images exist: {len(df)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category distribution\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "counts = df[\"articleType\"].value_counts()\n",
    "counts.plot(kind=\"barh\", ax=ax)\n",
    "ax.set_xlabel(\"Number of Images\")\n",
    "ax.set_title(\"Category Distribution (Filtered)\")\n",
    "plt.tight_layout()\n",
    "fig.savefig(FIGURES_DIR / \"category_distribution.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample images\n",
    "from PIL import Image\n",
    "from src.config import IMAGE_SIZE\n",
    "\n",
    "sample_cats = counts.head(6).index.tolist()\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "for ax, cat in zip(axes.flat, sample_cats):\n",
    "    sample_id = df[df[\"articleType\"] == cat].iloc[0][\"id\"]\n",
    "    img = Image.open(IMAGES_DIR / f\"{sample_id}.jpg\")\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(cat)\n",
    "    ax.axis(\"off\")\n",
    "plt.suptitle(\"Sample Images by Category\")\n",
    "plt.tight_layout()\n",
    "fig.savefig(FIGURES_DIR / \"sample_images.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, label_to_idx, idx_to_label = encode_labels(df)\n",
    "class_names = [idx_to_label[i] for i in range(len(idx_to_label))]\n",
    "num_classes = len(class_names)\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"Class names: {class_names}\")\n",
    "\n",
    "train_df, val_df, test_df = split_data(df)\n",
    "print(f\"\\nTrain: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = build_dataloaders(\n",
    "    train_df, val_df, test_df,\n",
    "    images_dir=IMAGES_DIR,\n",
    "    batch_size=DEFAULT_BATCH_SIZE,\n",
    "    num_workers=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train All Three Models\n",
    "\n",
    "Each model gets its own W&B run for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = {}\n",
    "\n",
    "model_configs = [\n",
    "    {\"name\": \"simple_cnn\", \"pretrained\": False, \"lr\": 1e-3, \"epochs\": DEFAULT_EPOCHS},\n",
    "    {\"name\": \"resnet50\", \"pretrained\": True, \"lr\": 1e-4, \"epochs\": DEFAULT_EPOCHS},\n",
    "    {\"name\": \"efficientnet_b0\", \"pretrained\": True, \"lr\": 1e-4, \"epochs\": DEFAULT_EPOCHS},\n",
    "]\n",
    "\n",
    "for cfg in model_configs:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training: {cfg['name']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    run = init_wandb_run(\n",
    "        config={\n",
    "            \"model_name\": cfg[\"name\"],\n",
    "            \"learning_rate\": cfg[\"lr\"],\n",
    "            \"batch_size\": DEFAULT_BATCH_SIZE,\n",
    "            \"dropout\": DEFAULT_DROPOUT,\n",
    "            \"epochs\": cfg[\"epochs\"],\n",
    "            \"num_classes\": num_classes,\n",
    "        },\n",
    "        project=WANDB_PROJECT,\n",
    "        name=f\"{cfg['name']}_baseline\",\n",
    "    )\n",
    "\n",
    "    model = build_model(\n",
    "        cfg[\"name\"],\n",
    "        num_classes=num_classes,\n",
    "        dropout=DEFAULT_DROPOUT,\n",
    "        pretrained=cfg.get(\"pretrained\", False),\n",
    "    )\n",
    "\n",
    "    results = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        num_epochs=cfg[\"epochs\"],\n",
    "        lr=cfg[\"lr\"],\n",
    "        device=device,\n",
    "        use_wandb=True,\n",
    "        class_names=class_names,\n",
    "    )\n",
    "\n",
    "    all_results[cfg[\"name\"]] = results\n",
    "    print(f\"Best val accuracy: {results['best_val_accuracy']:.4f}\")\n",
    "    wandb.finish()\n",
    "\n",
    "print(\"\\nAll training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison table\n",
    "comparison = pd.DataFrame(all_results).T\n",
    "comparison.index.name = \"Model\"\n",
    "print(comparison.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart of validation accuracy\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "models = list(all_results.keys())\n",
    "val_accs = [all_results[m][\"best_val_accuracy\"] for m in models]\n",
    "ax.bar(models, val_accs)\n",
    "ax.set_ylabel(\"Best Validation Accuracy\")\n",
    "ax.set_title(\"Model Comparison: Validation Accuracy\")\n",
    "ax.set_ylim(0, 1)\n",
    "for i, v in enumerate(val_accs):\n",
    "    ax.text(i, v + 0.01, f\"{v:.3f}\", ha=\"center\")\n",
    "plt.tight_layout()\n",
    "fig.savefig(FIGURES_DIR / \"model_comparison.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Set Evaluation (Best Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain or load best model and evaluate on test set\n",
    "best_model_name = max(all_results, key=lambda m: all_results[m][\"best_val_accuracy\"])\n",
    "print(f\"Best model: {best_model_name}\")\n",
    "\n",
    "best_model = build_model(\n",
    "    best_model_name,\n",
    "    num_classes=num_classes,\n",
    "    pretrained=best_model_name != \"simple_cnn\",\n",
    ")\n",
    "\n",
    "# Quick retrain of best model for test evaluation\n",
    "best_cfg = next(c for c in model_configs if c[\"name\"] == best_model_name)\n",
    "_ = train_model(\n",
    "    model=best_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epochs=best_cfg[\"epochs\"],\n",
    "    lr=best_cfg[\"lr\"],\n",
    "    device=device,\n",
    "    use_wandb=False,\n",
    ")\n",
    "\n",
    "# Test set evaluation\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "test_loss, test_acc, y_true, y_pred = evaluate(best_model, test_loader, criterion, device)\n",
    "print(f\"\\nTest accuracy: {test_acc:.4f}\")\n",
    "print(f\"Test loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "report = compute_classification_report(y_true, y_pred, label_names=class_names)\n",
    "report_df = pd.DataFrame(report).T\n",
    "print(report_df.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix heatmap\n",
    "cm = compute_confusion_matrix(y_true, y_pred)\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=class_names, yticklabels=class_names, ax=ax)\n",
    "ax.set_xlabel(\"Predicted\")\n",
    "ax.set_ylabel(\"True\")\n",
    "ax.set_title(f\"Confusion Matrix: {best_model_name}\")\n",
    "plt.tight_layout()\n",
    "fig.savefig(FIGURES_DIR / \"confusion_matrix.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Error Analysis\n",
    "\n",
    "Which class pairs are most commonly confused?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find most confused pairs (off-diagonal elements)\n",
    "cm_copy = cm.copy()\n",
    "np.fill_diagonal(cm_copy, 0)\n",
    "\n",
    "# Top 5 confused pairs\n",
    "confused_pairs = []\n",
    "for _ in range(5):\n",
    "    i, j = np.unravel_index(cm_copy.argmax(), cm_copy.shape)\n",
    "    confused_pairs.append((class_names[i], class_names[j], cm_copy[i, j]))\n",
    "    cm_copy[i, j] = 0\n",
    "\n",
    "print(\"Most confused pairs:\")\n",
    "for true_cls, pred_cls, count in confused_pairs:\n",
    "    print(f\"  {true_cls} -> {pred_cls}: {count} misclassifications\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Hyperparameter Sweep Results\n",
    "\n",
    "After running `wandb sweep sweep.yaml` and `wandb agent <sweep-id>`, pull results via the W&B API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull sweep results from W&B API\n",
    "api = wandb.Api()\n",
    "runs = api.runs(WANDB_PROJECT)\n",
    "\n",
    "sweep_data = []\n",
    "for run in runs:\n",
    "    if run.state == \"finished\":\n",
    "        sweep_data.append({\n",
    "            \"name\": run.name,\n",
    "            \"model\": run.config.get(\"model_name\", \"unknown\"),\n",
    "            \"lr\": run.config.get(\"learning_rate\"),\n",
    "            \"batch_size\": run.config.get(\"batch_size\"),\n",
    "            \"dropout\": run.config.get(\"dropout\"),\n",
    "            \"val_accuracy\": run.summary.get(\"val/accuracy\"),\n",
    "        })\n",
    "\n",
    "sweep_df = pd.DataFrame(sweep_data)\n",
    "print(f\"Total finished runs: {len(sweep_df)}\")\n",
    "sweep_df.sort_values(\"val_accuracy\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best config per model\n",
    "if len(sweep_df) > 0:\n",
    "    best_per_model = sweep_df.loc[sweep_df.groupby(\"model\")[\"val_accuracy\"].idxmax()]\n",
    "    print(best_per_model[[\"model\", \"lr\", \"batch_size\", \"dropout\", \"val_accuracy\"]].to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusions\n",
    "\n",
    "**Research question:** How much does transfer learning improve over a from-scratch CNN, and which backbone performs best?\n",
    "\n",
    "*Fill in after runs complete:*\n",
    "\n",
    "- SimpleCNN (from scratch): __% validation accuracy\n",
    "- ResNet-50 (pretrained): __% validation accuracy  \n",
    "- EfficientNet-B0 (pretrained): __% validation accuracy\n",
    "\n",
    "**Key findings:**\n",
    "- Transfer learning provides a ___ percentage point improvement over training from scratch.\n",
    "- [Best model] achieves the highest accuracy while [comparison notes].\n",
    "- The most commonly confused categories are [X] and [Y], which makes sense because [reason].\n",
    "- Hyperparameter sweep found that [key insights about learning rate, batch size, etc.]."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
