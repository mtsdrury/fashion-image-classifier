{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Fashion Image Classifier: Colab Experiment Notebook\n\n**Research question:** How much does transfer learning improve over a from-scratch CNN for fashion product classification, and which pretrained backbone performs best?\n\n**Models:** SimpleCNN (from scratch), ResNet-50 (pretrained), EfficientNet-B0 (pretrained)\n\n**Dataset:** [Fashion Product Images (Small)](https://www.kaggle.com/datasets/paramaggarwal/fashion-product-images-small) -- ~25k product images across multiple categories\n\n**Tracking:** Weights & Biases for experiment logging and comparison"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Verify GPU is available (Runtime > Change runtime type > T4 GPU)\nimport torch\n\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"Memory: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")\nelse:\n    print(\"WARNING: No GPU detected. Training will be very slow.\")\n    print(\"Go to Runtime > Change runtime type > T4 GPU\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!pip install -q wandb\n\nimport wandb\nwandb.login()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Download Dataset\n\nUpload your `kaggle.json` API key file, or use the Google Drive alternative if you have already downloaded the dataset."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport shutil\nfrom google.colab import files\n\n# --- Option 1: Kaggle API (recommended) ---\nos.makedirs(os.path.expanduser(\"~/.kaggle\"), exist_ok=True)\nprint(\"Upload your kaggle.json file:\")\nuploaded = files.upload()\nfor fn in uploaded:\n    shutil.move(fn, os.path.expanduser(\"~/.kaggle/kaggle.json\"))\nos.chmod(os.path.expanduser(\"~/.kaggle/kaggle.json\"), 0o600)\n\n!mkdir -p /content/data/raw\n!kaggle datasets download -d paramaggarwal/fashion-product-images-small -p /content/data/raw --unzip\n\n# --- Option 2: Google Drive (uncomment if dataset is in your Drive) ---\n# from google.colab import drive\n# drive.mount(\"/content/drive\")\n# !cp -r \"/content/drive/MyDrive/fashion-product-images-small/\" /content/data/raw/"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Setup: Imports and Configuration"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import models, transforms\nfrom PIL import Image\nfrom pathlib import Path\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (\n    accuracy_score,\n    classification_report,\n    confusion_matrix,\n    top_k_accuracy_score,\n)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport wandb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Paths (Colab) ─────────────────────────────────────────────────────\nDATA_DIR = Path(\"/content/data/raw\")\nSTYLES_CSV = DATA_DIR / \"styles.csv\"\nRESULTS_DIR = Path(\"/content/results\")\nFIGURES_DIR = RESULTS_DIR / \"figures\"\n\n# Auto-detect image directory (dataset structure varies)\nif (DATA_DIR / \"images\" / \"images\").is_dir():\n    IMAGES_DIR = DATA_DIR / \"images\" / \"images\"\nelif (DATA_DIR / \"images\").is_dir():\n    IMAGES_DIR = DATA_DIR / \"images\"\nelse:\n    raise FileNotFoundError(f\"No images directory found in {DATA_DIR}. Check download.\")\n\nFIGURES_DIR.mkdir(parents=True, exist_ok=True)\n\n# ── Dataset constants ──────────────────────────────────────────────────\nIMAGE_SIZE = 224\nMIN_CATEGORY_COUNT = 500\n\n# ── Train / val / test fractions ───────────────────────────────────────\nTRAIN_FRAC = 0.7\nVAL_FRAC = 0.15\nTEST_FRAC = 0.15\n\n# ── ImageNet normalization ─────────────────────────────────────────────\nIMAGENET_MEAN = [0.485, 0.456, 0.406]\nIMAGENET_STD = [0.229, 0.224, 0.225]\n\n# ── Default hyperparameters ────────────────────────────────────────────\nDEFAULT_LR = 1e-3\nDEFAULT_BATCH_SIZE = 32\nDEFAULT_EPOCHS = 10\nDEFAULT_DROPOUT = 0.3\nDEFAULT_NUM_WORKERS = 2\n\n# ── W&B ────────────────────────────────────────────────────────────────\nWANDB_PROJECT = \"fashion-image-classifier\"\nWANDB_ENTITY = None\n\n# ── Random seed ────────────────────────────────────────────────────────\nSEED = 42\n\n# ── Training config ───────────────────────────────────────────────────\nNUM_EPOCHS = 10\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nprint(f\"Training for {NUM_EPOCHS} epochs per model\")\n\n# ── Verify data exists ────────────────────────────────────────────────\nassert STYLES_CSV.exists(), f\"styles.csv not found at {STYLES_CSV}\"\nassert IMAGES_DIR.exists(), f\"Images directory not found at {IMAGES_DIR}\"\nprint(f\"Data directory: {DATA_DIR}\")\nprint(f\"Images directory: {IMAGES_DIR}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Augmentation and normalization pipelines ──────────────────────────\n\ndef get_train_transforms(flip=True, rotation=15.0, color_jitter=True):\n    \"\"\"Build training transform pipeline with configurable augmentation.\"\"\"\n    steps = [transforms.Resize((IMAGE_SIZE, IMAGE_SIZE))]\n    if flip:\n        steps.append(transforms.RandomHorizontalFlip())\n    if rotation > 0:\n        steps.append(transforms.RandomRotation(rotation))\n    if color_jitter:\n        steps.append(\n            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2)\n        )\n    steps += [\n        transforms.ToTensor(),\n        transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n    ]\n    return transforms.Compose(steps)\n\n\ndef get_eval_transforms():\n    \"\"\"Resize and normalize only (no augmentation).\"\"\"\n    return transforms.Compose([\n        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n    ])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Data loading: metadata, filtering, splitting, DataLoaders ─────────\n\ndef load_metadata(csv_path=STYLES_CSV):\n    \"\"\"Load styles.csv and return a DataFrame with id and articleType.\"\"\"\n    df = pd.read_csv(csv_path, on_bad_lines=\"skip\")\n    df = df[[\"id\", \"articleType\"]].dropna()\n    df[\"id\"] = df[\"id\"].astype(int)\n    return df\n\n\ndef filter_top_categories(df, min_count=MIN_CATEGORY_COUNT):\n    \"\"\"Keep only categories with at least min_count images.\"\"\"\n    counts = df[\"articleType\"].value_counts()\n    keep = counts[counts >= min_count].index\n    return df[df[\"articleType\"].isin(keep)].reset_index(drop=True)\n\n\ndef verify_images_exist(df, images_dir=IMAGES_DIR):\n    \"\"\"Drop rows whose image file is missing on disk.\"\"\"\n    exists_mask = df[\"id\"].apply(\n        lambda pid: (images_dir / f\"{pid}.jpg\").is_file()\n    )\n    return df[exists_mask].reset_index(drop=True)\n\n\ndef encode_labels(df):\n    \"\"\"Add an integer label column; return label-to-index and index-to-label maps.\"\"\"\n    categories = sorted(df[\"articleType\"].unique())\n    label_to_idx = {cat: i for i, cat in enumerate(categories)}\n    idx_to_label = {i: cat for cat, i in label_to_idx.items()}\n    df = df.copy()\n    df[\"label\"] = df[\"articleType\"].map(label_to_idx)\n    return df, label_to_idx, idx_to_label\n\n\ndef split_data(df, train_frac=TRAIN_FRAC, val_frac=VAL_FRAC,\n               test_frac=TEST_FRAC, seed=SEED):\n    \"\"\"Stratified split into train, validation, and test sets.\"\"\"\n    assert abs(train_frac + val_frac + test_frac - 1.0) < 1e-6\n    train_df, temp_df = train_test_split(\n        df, test_size=(val_frac + test_frac),\n        stratify=df[\"label\"], random_state=seed,\n    )\n    relative_test = test_frac / (val_frac + test_frac)\n    val_df, test_df = train_test_split(\n        temp_df, test_size=relative_test,\n        stratify=temp_df[\"label\"], random_state=seed,\n    )\n    return (\n        train_df.reset_index(drop=True),\n        val_df.reset_index(drop=True),\n        test_df.reset_index(drop=True),\n    )\n\n\nclass FashionDataset(Dataset):\n    \"\"\"Map-style dataset that loads product images and returns (image, label).\"\"\"\n\n    def __init__(self, df, images_dir=IMAGES_DIR, transform=None):\n        self.df = df.reset_index(drop=True)\n        self.images_dir = images_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        img_path = self.images_dir / f\"{row['id']}.jpg\"\n        image = Image.open(img_path).convert(\"RGB\")\n        if self.transform:\n            image = self.transform(image)\n        return image, int(row[\"label\"])\n\n\ndef build_dataloaders(train_df, val_df, test_df, images_dir=IMAGES_DIR,\n                      batch_size=DEFAULT_BATCH_SIZE, num_workers=DEFAULT_NUM_WORKERS,\n                      augment_flip=True, augment_rotation=15.0,\n                      augment_color_jitter=True):\n    \"\"\"Create train, validation, and test DataLoaders.\"\"\"\n    train_transform = get_train_transforms(\n        flip=augment_flip, rotation=augment_rotation, color_jitter=augment_color_jitter,\n    )\n    eval_transform = get_eval_transforms()\n\n    train_ds = FashionDataset(train_df, images_dir, transform=train_transform)\n    val_ds = FashionDataset(val_df, images_dir, transform=eval_transform)\n    test_ds = FashionDataset(test_df, images_dir, transform=eval_transform)\n\n    train_loader = DataLoader(\n        train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers,\n    )\n    val_loader = DataLoader(\n        val_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers,\n    )\n    test_loader = DataLoader(\n        test_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers,\n    )\n    return train_loader, val_loader, test_loader"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Model definitions: SimpleCNN, ResNet-50, EfficientNet-B0 ──────────\n\nclass SimpleCNN(nn.Module):\n    \"\"\"3-block CNN trained from scratch.\"\"\"\n\n    def __init__(self, num_classes, dropout=DEFAULT_DROPOUT):\n        super().__init__()\n        self.features = nn.Sequential(\n            # Block 1\n            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            # Block 2\n            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            # Block 3\n            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.AdaptiveAvgPool2d(1),\n        )\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Dropout(dropout),\n            nn.Linear(128, num_classes),\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        return self.classifier(x)\n\n\nclass ResNet50Classifier(nn.Module):\n    \"\"\"ResNet-50 with pretrained ImageNet backbone and replaced FC head.\"\"\"\n\n    def __init__(self, num_classes, dropout=DEFAULT_DROPOUT, pretrained=True):\n        super().__init__()\n        weights = models.ResNet50_Weights.DEFAULT if pretrained else None\n        self.backbone = models.resnet50(weights=weights)\n        in_features = self.backbone.fc.in_features\n        self.backbone.fc = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Linear(in_features, num_classes),\n        )\n\n    def forward(self, x):\n        return self.backbone(x)\n\n\nclass EfficientNetB0Classifier(nn.Module):\n    \"\"\"EfficientNet-B0 with pretrained ImageNet backbone and replaced classifier.\"\"\"\n\n    def __init__(self, num_classes, dropout=DEFAULT_DROPOUT, pretrained=True):\n        super().__init__()\n        weights = models.EfficientNet_B0_Weights.DEFAULT if pretrained else None\n        self.backbone = models.efficientnet_b0(weights=weights)\n        in_features = self.backbone.classifier[1].in_features\n        self.backbone.classifier = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Linear(in_features, num_classes),\n        )\n\n    def forward(self, x):\n        return self.backbone(x)\n\n\nMODEL_REGISTRY = {\n    \"simple_cnn\": SimpleCNN,\n    \"resnet50\": ResNet50Classifier,\n    \"efficientnet_b0\": EfficientNetB0Classifier,\n}\n\n\ndef build_model(name, num_classes, **kwargs):\n    \"\"\"Instantiate a model by registry name.\"\"\"\n    if name not in MODEL_REGISTRY:\n        raise ValueError(\n            f\"Unknown model '{name}'. Choose from: {list(MODEL_REGISTRY.keys())}\"\n        )\n    # SimpleCNN doesn't accept 'pretrained', so drop it\n    if name == \"simple_cnn\":\n        kwargs.pop(\"pretrained\", None)\n    return MODEL_REGISTRY[name](num_classes=num_classes, **kwargs)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Pure evaluation functions ─────────────────────────────────────────\n\ndef compute_accuracy(y_true, y_pred):\n    \"\"\"Overall accuracy.\"\"\"\n    return float(accuracy_score(y_true, y_pred))\n\n\ndef compute_classification_report(y_true, y_pred, label_names=None):\n    \"\"\"Per-class precision, recall, F1 as a dict.\"\"\"\n    return classification_report(\n        y_true, y_pred, target_names=label_names, output_dict=True, zero_division=0\n    )\n\n\ndef compute_confusion_matrix(y_true, y_pred):\n    \"\"\"Confusion matrix as a 2-D numpy array.\"\"\"\n    return confusion_matrix(y_true, y_pred)\n\n\ndef compute_topk_accuracy(y_true, y_probs, k=3):\n    \"\"\"Top-k accuracy from probability matrix.\"\"\"\n    labels = list(range(y_probs.shape[1]))\n    return float(top_k_accuracy_score(y_true, y_probs, k=k, labels=labels))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── W&B helper functions ──────────────────────────────────────────────\n\ndef _denormalize(tensor):\n    \"\"\"Convert a normalized image tensor back to uint8 HWC numpy array for display.\"\"\"\n    mean = torch.tensor(IMAGENET_MEAN).view(3, 1, 1)\n    std = torch.tensor(IMAGENET_STD).view(3, 1, 1)\n    img = tensor.cpu() * std + mean\n    img = img.clamp(0, 1).permute(1, 2, 0).numpy()\n    return (img * 255).astype(np.uint8)\n\n\ndef init_wandb_run(config, project, entity=None, **kwargs):\n    \"\"\"Initialize a W&B run. Returns the run object.\"\"\"\n    return wandb.init(project=project, entity=entity, config=config, **kwargs)\n\n\ndef log_confusion_matrix(y_true, y_pred, class_names, title=\"Confusion Matrix\"):\n    \"\"\"Log a confusion matrix to the current W&B run.\"\"\"\n    wandb.log(\n        {\n            title: wandb.plot.confusion_matrix(\n                probs=None, y_true=y_true, preds=y_pred, class_names=class_names\n            )\n        }\n    )\n\n\ndef log_sample_predictions(images, y_true, y_pred, confidences, class_names,\n                           max_images=16):\n    \"\"\"Log a grid of sample predictions (correct and incorrect) to W&B.\"\"\"\n    table = wandb.Table(columns=[\"image\", \"true\", \"predicted\", \"confidence\", \"correct\"])\n    n = min(len(y_true), max_images)\n    for i in range(n):\n        img_array = _denormalize(images[i])\n        table.add_data(\n            wandb.Image(img_array),\n            class_names[y_true[i]],\n            class_names[y_pred[i]],\n            f\"{confidences[i]:.3f}\",\n            y_true[i] == y_pred[i],\n        )\n    wandb.log({\"sample_predictions\": table})"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Training and evaluation loops ─────────────────────────────────────\n\ndef train_one_epoch(model, loader, criterion, optimizer, device):\n    \"\"\"Run one training epoch. Returns (avg_loss, accuracy).\"\"\"\n    model.train()\n    total_loss = 0.0\n    all_preds, all_labels = [], []\n\n    for images, labels in loader:\n        images, labels = images.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item() * images.size(0)\n        preds = outputs.argmax(dim=1).cpu().tolist()\n        all_preds.extend(preds)\n        all_labels.extend(labels.cpu().tolist())\n\n    avg_loss = total_loss / len(all_labels)\n    accuracy = compute_accuracy(all_labels, all_preds)\n    return avg_loss, accuracy\n\n\n@torch.no_grad()\ndef evaluate(model, loader, criterion, device):\n    \"\"\"Evaluate model on a data loader. Returns (avg_loss, accuracy, y_true, y_pred).\"\"\"\n    model.eval()\n    total_loss = 0.0\n    all_preds, all_labels = [], []\n\n    for images, labels in loader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n\n        total_loss += loss.item() * images.size(0)\n        preds = outputs.argmax(dim=1).cpu().tolist()\n        all_preds.extend(preds)\n        all_labels.extend(labels.cpu().tolist())\n\n    avg_loss = total_loss / len(all_labels)\n    accuracy = compute_accuracy(all_labels, all_preds)\n    return avg_loss, accuracy, all_labels, all_preds\n\n\ndef train_model(model, train_loader, val_loader, num_epochs=DEFAULT_EPOCHS,\n                lr=DEFAULT_LR, device=None, use_wandb=False, class_names=None):\n    \"\"\"Full training pipeline with optional W&B logging.\n\n    Returns a dict with final train/val loss and accuracy.\n    \"\"\"\n    if device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\n    best_val_acc = 0.0\n\n    for epoch in range(num_epochs):\n        train_loss, train_acc = train_one_epoch(\n            model, train_loader, criterion, optimizer, device\n        )\n        val_loss, val_acc, val_true, val_pred = evaluate(\n            model, val_loader, criterion, device\n        )\n\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n\n        print(f\"  Epoch {epoch + 1}/{num_epochs} | \"\n              f\"train_loss: {train_loss:.4f}, train_acc: {train_acc:.4f} | \"\n              f\"val_loss: {val_loss:.4f}, val_acc: {val_acc:.4f}\")\n\n        if use_wandb:\n            wandb.log(\n                {\n                    \"epoch\": epoch + 1,\n                    \"train/loss\": train_loss,\n                    \"train/accuracy\": train_acc,\n                    \"val/loss\": val_loss,\n                    \"val/accuracy\": val_acc,\n                }\n            )\n\n    # Log final confusion matrix\n    if use_wandb and class_names:\n        log_confusion_matrix(val_true, val_pred, class_names, title=\"val/confusion_matrix\")\n\n    return {\n        \"train_loss\": train_loss,\n        \"train_accuracy\": train_acc,\n        \"val_loss\": val_loss,\n        \"val_accuracy\": val_acc,\n        \"best_val_accuracy\": best_val_acc,\n    }"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Data Exploration"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load and filter metadata\ndf = load_metadata(STYLES_CSV)\nprint(f\"Total rows in styles.csv: {len(df)}\")\nprint(f\"Unique article types: {df['articleType'].nunique()}\")\n\ndf = filter_top_categories(df)\nprint(f\"\\nAfter filtering (>= 500 images): {len(df)} rows, {df['articleType'].nunique()} categories\")\n\ndf = verify_images_exist(df, IMAGES_DIR)\nprint(f\"After verifying images exist: {len(df)} rows\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Category distribution\nfig, ax = plt.subplots(figsize=(12, 6))\ncounts = df[\"articleType\"].value_counts()\ncounts.plot(kind=\"barh\", ax=ax)\nax.set_xlabel(\"Number of Images\")\nax.set_title(\"Category Distribution (Filtered)\")\nplt.tight_layout()\nfig.savefig(FIGURES_DIR / \"category_distribution.png\", dpi=150)\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Sample images\nsample_cats = counts.head(6).index.tolist()\nfig, axes = plt.subplots(2, 3, figsize=(12, 8))\nfor ax, cat in zip(axes.flat, sample_cats):\n    sample_id = df[df[\"articleType\"] == cat].iloc[0][\"id\"]\n    img = Image.open(IMAGES_DIR / f\"{sample_id}.jpg\")\n    ax.imshow(img)\n    ax.set_title(cat)\n    ax.axis(\"off\")\nplt.suptitle(\"Sample Images by Category\")\nplt.tight_layout()\nfig.savefig(FIGURES_DIR / \"sample_images.png\", dpi=150)\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Data Preparation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "df, label_to_idx, idx_to_label = encode_labels(df)\nclass_names = [idx_to_label[i] for i in range(len(idx_to_label))]\nnum_classes = len(class_names)\nprint(f\"Number of classes: {num_classes}\")\nprint(f\"Class names: {class_names}\")\n\ntrain_df, val_df, test_df = split_data(df)\nprint(f\"\\nTrain: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "train_loader, val_loader, test_loader = build_dataloaders(\n    train_df, val_df, test_df,\n    images_dir=IMAGES_DIR,\n    batch_size=DEFAULT_BATCH_SIZE,\n    num_workers=2,\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Train All Three Models\n\nEach model gets its own W&B run for comparison."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "all_results = {}\ntrained_models = {}\n\nmodel_configs = [\n    {\"name\": \"simple_cnn\", \"pretrained\": False, \"lr\": 1e-3, \"epochs\": NUM_EPOCHS},\n    {\"name\": \"resnet50\", \"pretrained\": True, \"lr\": 1e-4, \"epochs\": NUM_EPOCHS},\n    {\"name\": \"efficientnet_b0\", \"pretrained\": True, \"lr\": 1e-4, \"epochs\": NUM_EPOCHS},\n]\n\nfor cfg in model_configs:\n    print(f\"\\n{'='*60}\")\n    print(f\"Training: {cfg['name']}\")\n    print(f\"{'='*60}\")\n\n    run = init_wandb_run(\n        config={\n            \"model_name\": cfg[\"name\"],\n            \"learning_rate\": cfg[\"lr\"],\n            \"batch_size\": DEFAULT_BATCH_SIZE,\n            \"dropout\": DEFAULT_DROPOUT,\n            \"epochs\": cfg[\"epochs\"],\n            \"num_classes\": num_classes,\n        },\n        project=WANDB_PROJECT,\n        name=f\"{cfg['name']}_baseline\",\n    )\n\n    model = build_model(\n        cfg[\"name\"],\n        num_classes=num_classes,\n        dropout=DEFAULT_DROPOUT,\n        pretrained=cfg.get(\"pretrained\", False),\n    )\n\n    results = train_model(\n        model=model,\n        train_loader=train_loader,\n        val_loader=val_loader,\n        num_epochs=cfg[\"epochs\"],\n        lr=cfg[\"lr\"],\n        device=device,\n        use_wandb=True,\n        class_names=class_names,\n    )\n\n    all_results[cfg[\"name\"]] = results\n    trained_models[cfg[\"name\"]] = model\n    print(f\"Best val accuracy: {results['best_val_accuracy']:.4f}\")\n    wandb.finish()\n\nprint(\"\\nAll training complete.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Model Comparison"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Comparison table\ncomparison = pd.DataFrame(all_results).T\ncomparison.index.name = \"Model\"\nprint(comparison.to_markdown())"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Bar chart of validation accuracy\nfig, ax = plt.subplots(figsize=(8, 5))\nmodel_names = list(all_results.keys())\nval_accs = [all_results[m][\"best_val_accuracy\"] for m in model_names]\nax.bar(model_names, val_accs)\nax.set_ylabel(\"Best Validation Accuracy\")\nax.set_title(\"Model Comparison: Validation Accuracy\")\nax.set_ylim(0, 1)\nfor i, v in enumerate(val_accs):\n    ax.text(i, v + 0.01, f\"{v:.3f}\", ha=\"center\")\nplt.tight_layout()\nfig.savefig(FIGURES_DIR / \"model_comparison.png\", dpi=150)\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Test Set Evaluation (Best Model)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Use already-trained best model for test evaluation (no retrain)\nbest_model_name = max(all_results, key=lambda m: all_results[m][\"best_val_accuracy\"])\nprint(f\"Best model: {best_model_name}\")\n\nbest_model = trained_models[best_model_name]\n\n# Test set evaluation\ncriterion = torch.nn.CrossEntropyLoss()\ntest_loss, test_acc, y_true, y_pred = evaluate(best_model, test_loader, criterion, device)\nprint(f\"\\nTest accuracy: {test_acc:.4f}\")\nprint(f\"Test loss: {test_loss:.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Classification report\nreport = compute_classification_report(y_true, y_pred, label_names=class_names)\nreport_df = pd.DataFrame(report).T\nprint(report_df.to_markdown())"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Confusion matrix heatmap\ncm = compute_confusion_matrix(y_true, y_pred)\nfig, ax = plt.subplots(figsize=(12, 10))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n            xticklabels=class_names, yticklabels=class_names, ax=ax)\nax.set_xlabel(\"Predicted\")\nax.set_ylabel(\"True\")\nax.set_title(f\"Confusion Matrix: {best_model_name}\")\nplt.tight_layout()\nfig.savefig(FIGURES_DIR / \"confusion_matrix.png\", dpi=150)\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Error Analysis\n\nWhich class pairs are most commonly confused?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Find most confused pairs (off-diagonal elements)\ncm_copy = cm.copy()\nnp.fill_diagonal(cm_copy, 0)\n\n# Top 5 confused pairs\nconfused_pairs = []\nfor _ in range(5):\n    i, j = np.unravel_index(cm_copy.argmax(), cm_copy.shape)\n    confused_pairs.append((class_names[i], class_names[j], cm_copy[i, j]))\n    cm_copy[i, j] = 0\n\nprint(\"Most confused pairs:\")\nfor true_cls, pred_cls, count in confused_pairs:\n    print(f\"  {true_cls} -> {pred_cls}: {count} misclassifications\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Hyperparameter Sweep Results\n\nAfter running `wandb sweep sweep.yaml` and `wandb agent <sweep-id>`, pull results via the W&B API."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Pull sweep results from W&B API (skip if no sweep runs yet)\ntry:\n    api = wandb.Api()\n    runs = api.runs(WANDB_PROJECT)\n\n    sweep_data = []\n    for run in runs:\n        if run.state == \"finished\":\n            sweep_data.append({\n                \"name\": run.name,\n                \"model\": run.config.get(\"model_name\", \"unknown\"),\n                \"lr\": run.config.get(\"learning_rate\"),\n                \"batch_size\": run.config.get(\"batch_size\"),\n                \"dropout\": run.config.get(\"dropout\"),\n                \"val_accuracy\": run.summary.get(\"val/accuracy\"),\n            })\n\n    sweep_df = pd.DataFrame(sweep_data)\n    print(f\"Total finished runs: {len(sweep_df)}\")\n    if len(sweep_df) > 0:\n        print(sweep_df.sort_values(\"val_accuracy\", ascending=False).head(10).to_markdown())\n    else:\n        print(\"No sweep runs found yet. Run wandb sweep + wandb agent to populate.\")\nexcept Exception as e:\n    print(f\"Could not fetch sweep results: {e}\")\n    sweep_df = pd.DataFrame()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Best config per model\nif len(sweep_df) > 0:\n    best_per_model = sweep_df.loc[sweep_df.groupby(\"model\")[\"val_accuracy\"].idxmax()]\n    print(best_per_model[[\"model\", \"lr\", \"batch_size\", \"dropout\", \"val_accuracy\"]].to_markdown())"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. Conclusions\n\n**Research question:** How much does transfer learning improve over a from-scratch CNN, and which backbone performs best?\n\n**Validation accuracy (best epoch):**\n\n- SimpleCNN (from scratch): **69.1%**\n- ResNet-50 (pretrained): **93.3%**\n- EfficientNet-B0 (pretrained): **93.5%**\n\n**Test set evaluation (EfficientNet-B0):** 93.3% accuracy, 0.219 loss\n\n**Key findings:**\n\n- Transfer learning provides a **~24 percentage point improvement** over training from scratch (93.5% vs. 69.1%), confirming that pretrained ImageNet features transfer well to fashion product classification.\n- EfficientNet-B0 narrowly outperforms ResNet-50 (93.5% vs. 93.3% validation accuracy) while using a significantly smaller backbone (5.3M vs. 25.6M parameters), making it the better choice on both accuracy and efficiency.\n- Both pretrained models converge quickly, reaching >90% validation accuracy within the first epoch, while SimpleCNN is still below 70% after 10 epochs and shows unstable validation loss (spiking to 8.1 in epoch 3).\n- The most commonly confused category pairs are visually similar items: Casual Shoes vs. Sports Shoes (48 + 30 = 78 bidirectional errors), Flats vs. Heels (46 errors), and Tops vs. Tshirts (45 + 32 = 77 bidirectional errors). These confusions make sense because these categories share similar shapes, colors, and textures.\n- Per-class performance varies widely: Briefs and Sunglasses achieve perfect F1 scores (1.0), while Flats is the hardest category (F1 = 0.39, recall = 0.31), likely because Flats get confused with Heels, Sandals, and Casual Shoes.\n- No dedicated hyperparameter sweep was run beyond the three baseline configurations. All runs used dropout = 0.3 and batch size = 32, with lr = 1e-3 for SimpleCNN and lr = 1e-4 for the pretrained models."
  }
 ]
}